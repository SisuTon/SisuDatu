import os
from collections import defaultdict

def find_duplicates(root_dir):
    """–ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Ñ–∞–π–ª–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É."""
    hashes = defaultdict(list)
    for dirpath, _, filenames in os.walk(root_dir):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.getsize(filepath) == 0:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
                hashes["empty"].append(filepath)
    return hashes

def find_large_files(root_dir, size_threshold_kb=100):
    """–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã –±–æ–ª—å—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."""
    large_files = []
    for dirpath, _, filenames in os.walk(root_dir):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            size_kb = os.path.getsize(filepath) / 1024
            if size_kb > size_threshold_kb:
                large_files.append((filepath, f"{size_kb:.1f} KB"))
    return large_files

def clean_duplicates(duplicates, keep_patterns=None):
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è —Ñ–∞–π–ª—ã –ø–æ —à–∞–±–ª–æ–Ω–∞–º."""
    if keep_patterns is None:
        keep_patterns = ["__init__.py"]  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ __init__.py
    
    to_delete = []
    for filepath in duplicates["empty"]:
        filename = os.path.basename(filepath)
        if any(pattern in filename for pattern in keep_patterns):
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ —à–∞–±–ª–æ–Ω–∞–º
        to_delete.append(filepath)
    
    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
    for filepath in to_delete:
        try:
            os.remove(filepath)
            print(f"–£–¥–∞–ª—ë–Ω: {filepath}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ {filepath}: {e}")
    
    return len(to_delete)

def analyze_project(root_dir="."):
    """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏."""
    print("üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞...")
    
    # 1. –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicates = find_duplicates(root_dir)
    print(f"\nüìå –ù–∞–π–¥–µ–Ω–æ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(duplicates['empty'])}")
    
    # 2. –ü–æ–∏—Å–∫ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
    large_files = find_large_files(root_dir)
    print(f"\nüìå –§–∞–π–ª—ã >100 KB:")
    for file, size in large_files:
        print(f"  - {file} ({size})")
    
    # 3. –û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º __init__.py)
    print("\nüßπ –û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    deleted_count = clean_duplicates(duplicates)
    print(f"–£–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {deleted_count}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    if large_files:
        print("1. –†–∞–∑–¥–µ–ª–∏—Ç–µ –±–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, ai_handler.py) –Ω–∞ –ø–æ–¥–º–æ–¥—É–ª–∏")
    if len(duplicates["empty"]) - deleted_count > 0:
        print("2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—É—Å—Ç—ã–µ __init__.py - –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å __all__ –∏–ª–∏ docstring")

if __name__ == "__main__":
    project_root = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–µ–∫—É—â–∞—è –ø–∞–ø–∫–∞): ").strip() or "."
    analyze_project(project_root) 